{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.layers as tkl\nimport cv2\nfrom skimage.util import random_noise\nfrom sklearn.model_selection import train_test_split\nfrom itertools import chain\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport random\nimport glob\nimport os\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! cd . ; pwd; ls; \n!cd lfw_funneled; pwd; ls","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:02:44.636311Z","iopub.execute_input":"2021-12-19T07:02:44.636586Z","iopub.status.idle":"2021-12-19T07:02:46.012937Z","shell.execute_reply.started":"2021-12-19T07:02:44.636551Z","shell.execute_reply":"2021-12-19T07:02:46.012118Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!tar -xvf '../input/lfwpeople/lfw-funneled.tgz' -C './'","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-19T07:02:46.015964Z","iopub.execute_input":"2021-12-19T07:02:46.016197Z","iopub.status.idle":"2021-12-19T07:02:49.854347Z","shell.execute_reply.started":"2021-12-19T07:02:46.016168Z","shell.execute_reply":"2021-12-19T07:02:49.853569Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"source_path = './lfw_funneled'\n# destination_path = './Pdataset'\nfilenames = []\n\n# for file in source_path:\n# #    filenames.append(glob.glob('source_path/file/*jpg'))\n#     print(file)\n# len(filenames)\nfile_path = os.listdir(source_path)\n\nfor file in file_path:\n    \n    filenames.append(glob.glob(os.path.join(source_path, f'{file}/*.jpg')))\n\nfilenames = list(chain.from_iterable(filenames))\nrandom.shuffle(filenames)\nfilenames_backup = filenames","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:02:49.857027Z","iopub.execute_input":"2021-12-19T07:02:49.857300Z","iopub.status.idle":"2021-12-19T07:02:50.168536Z","shell.execute_reply.started":"2021-12-19T07:02:49.857261Z","shell.execute_reply":"2021-12-19T07:02:50.167585Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Showing an image before and after adding some noises:","metadata":{}},{"cell_type":"code","source":"img_path = filenames[1000]\nprint(img_path)\nimg = cv2.imread(img_path)\n# img = img.astype(np.float64) / 255\n# img = Image.convert(img, )\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nnoisy = random_noise(img, mode = 's&p', amount=0.2)\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(img)\nax[0].set_title('Original')\nax[1].imshow(noisy)\nax[1].set_title('Noisy')\n# plt.imshow(noisy)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:02:50.173930Z","iopub.execute_input":"2021-12-19T07:02:50.174148Z","iopub.status.idle":"2021-12-19T07:02:50.669396Z","shell.execute_reply.started":"2021-12-19T07:02:50.174117Z","shell.execute_reply":"2021-12-19T07:02:50.668658Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Length of the dataset:","metadata":{}},{"cell_type":"code","source":"print(np.array(filenames).shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:02:50.672621Z","iopub.execute_input":"2021-12-19T07:02:50.673475Z","iopub.status.idle":"2021-12-19T07:02:50.694430Z","shell.execute_reply.started":"2021-12-19T07:02:50.673430Z","shell.execute_reply":"2021-12-19T07:02:50.693491Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Spliting the train and test set:","metadata":{}},{"cell_type":"code","source":"test_filenames = filenames[-4001:-1]\ndel filenames[-4001:-1]\ntrain_filenames = filenames\nprint(f'Size of training set is: {len(train_filenames)}', end='\\n----------------------------\\n')\nprint(f'Size of test set is: {len(test_filenames)}', end='\\n----------------------------\\n')","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:02:50.699508Z","iopub.execute_input":"2021-12-19T07:02:50.702209Z","iopub.status.idle":"2021-12-19T07:02:50.713456Z","shell.execute_reply.started":"2021-12-19T07:02:50.702161Z","shell.execute_reply":"2021-12-19T07:02:50.712572Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_img, val_img, _, _ = train_test_split(train_filenames, train_filenames, test_size = 0.2, shuffle = True, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:03:24.655147Z","iopub.execute_input":"2021-12-19T07:03:24.655550Z","iopub.status.idle":"2021-12-19T07:03:24.674716Z","shell.execute_reply.started":"2021-12-19T07:03:24.655506Z","shell.execute_reply":"2021-12-19T07:03:24.673949Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(f'Size of training set is: {len(train_img)}', end='\\n----------------------------\\n')\nprint(f'Size of validation set is: {len(val_img)}', end='\\n----------------------------\\n')\nprint(f'Total: {len(train_img) + len(val_img)}')","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:03:25.003123Z","iopub.execute_input":"2021-12-19T07:03:25.003383Z","iopub.status.idle":"2021-12-19T07:03:25.009649Z","shell.execute_reply.started":"2021-12-19T07:03:25.003356Z","shell.execute_reply":"2021-12-19T07:03:25.008780Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"WIDTH = 256\nHEIGHT = 256\nn_channels = 3\nBATCH_SIZE = 128","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:03:25.033648Z","iopub.execute_input":"2021-12-19T07:03:25.034098Z","iopub.status.idle":"2021-12-19T07:03:25.037917Z","shell.execute_reply.started":"2021-12-19T07:03:25.034069Z","shell.execute_reply":"2021-12-19T07:03:25.037037Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class  DataGenerator(tf.keras.utils.Sequence):\n    \n    def __init__(self, orig_filenames, batch_size = BATCH_SIZE, shuffle = True):\n        self.orig_filenames = orig_filenames\n        self.noisy_filenames = orig_filenames\n        self.filenames = list(zip(self.orig_filenames, self.noisy_filenames))\n        self.batch_size = BATCH_SIZE\n        self.shuffle = shuffle\n    \n    def __len__(self):\n        return (len(self.orig_filenames) // self.batch_size)\n    \n    def __getitem__(self, idx):\n        batch = self.filenames[idx * self.batch_size : (idx + 1) * self.batch_size]\n        X, Y = self.__data_generation(batch)\n\n        return X, Y\n    \n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.orig_filenames))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n            \n    def __data_generation(self, batch):\n        orig = []\n        noisy = []\n        \n        for orig_file, _ in batch:\n          # image = self.colored_image_loader(orig_file)\n          image = cv2.imread(orig_file) / 255\n#           image = tf.image.convert_image_dtype(image, tf.float32) / 255\n          image = cv2.resize(image, (WIDTH, HEIGHT))\n#           image = tf.image.convert_image_dtype(image, tf.float32)\n            \n          # noisy = cv2.imread(mask_file)\n          # mask = cv2.resize(mask, (WIDTH, HEIGHT))[:, :, 2]\n\n#           orig.append(cv2.resize(image, (252, 252)))\n          orig.append(image)\n          \n          noisy.append(self.make_noisy(image))\n          \n        return  np.array(noisy), np.array(orig)\n\n    # def colored_image_loader(self, image_filename):\n    #   img = cv2.imread(image_filename)\n    #   img = cv2.resize(img, (WIDTH, HEIGHT))\n    #   img = tf.image.convert_image_dtype(img, tf.float32)\n      \n      # return img\n\n    def make_noisy(self, image):\n      # image = cv2.imread(img_file)\n      image_ = image\n      noisy_image = random_noise(image_, mode = 's&p', amount = 0.2)\n      \n      return noisy_image","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:04:20.477365Z","iopub.execute_input":"2021-12-19T07:04:20.477635Z","iopub.status.idle":"2021-12-19T07:04:20.491455Z","shell.execute_reply.started":"2021-12-19T07:04:20.477605Z","shell.execute_reply":"2021-12-19T07:04:20.490676Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# class  DataGenerator(tf.keras.utils.Sequence):\n    \n#     def __init__(self, orig_filenames, batch_size = BATCH_SIZE, shuffle = True):\n\ntrain_generator = DataGenerator(train_img, BATCH_SIZE)\nval_generator = DataGenerator(val_img, BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:04:32.174565Z","iopub.execute_input":"2021-12-19T07:04:32.175094Z","iopub.status.idle":"2021-12-19T07:04:32.180337Z","shell.execute_reply.started":"2021-12-19T07:04:32.175061Z","shell.execute_reply":"2021-12-19T07:04:32.179457Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Model\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Input, Conv2D, UpSampling2D, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:04:34.132000Z","iopub.execute_input":"2021-12-19T07:04:34.132332Z","iopub.status.idle":"2021-12-19T07:04:34.138038Z","shell.execute_reply.started":"2021-12-19T07:04:34.132293Z","shell.execute_reply":"2021-12-19T07:04:34.137257Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.layers as tkl\ndef conv_block(inputs = None, n_filters = 32, kernel_size = 3, batch_norm = True):\n    \n    '''First layer'''\n    \n    conv = tkl.Conv2D(filters = n_filters,\n                      kernel_size = kernel_size,\n                      padding = 'same',\n                      kernel_initializer = 'he_normal')(inputs)\n    \n    if batch_norm:\n        conv = tkl.BatchNormalization()(conv)\n    \n    conv = tkl.Activation('relu')(conv)\n    \n    '''Second layer'''\n    \n    conv = tkl.Conv2D(filters = n_filters,\n                     kernel_size = kernel_size,\n                     padding = 'same',\n                     kernel_initializer = 'he_normal')(conv)\n    \n    if batch_norm:\n        conv = tkl.BatchNormalization()(conv)\n    \n    conv = tkl.Activation('relu')(conv)\n        \n    return conv\n\n\ndef conv_transpose_block(n_filters, kernel_size = 3):\n    conv_transpose = tkl.Conv2DTranspose(n_filters, kernel_size = kernel_size, strides = (2,2), padding = 'same')\n    \n    return conv_transpose\n    \n\ndef u_net(n_filters = 32, dropout_prob = 0.1, batch_norm = True):\n    input_size = (WIDTH, HEIGHT, n_channels)\n    \n    input_img = tf.keras.Input(input_size, name = 'image' )\n    \n    c1 = conv_block(input_img, n_filters, kernel_size = 3)\n    p1 = tkl.MaxPooling2D((2,2))(c1)\n    p1 = tkl.Dropout(dropout_prob)(p1)\n    \n    c2 = conv_block(p1, n_filters * 2, kernel_size = 3)\n    p2 = tkl.MaxPooling2D((2,2))(c2)\n    p2 = tkl.Dropout(dropout_prob)(p2)\n    \n    c3 = conv_block(p2, n_filters * 4, kernel_size = 3)\n    p3 = tkl.MaxPooling2D((2,2))(c3)\n    p3 = tkl.Dropout(dropout_prob)(p3)\n    \n    c4 = conv_block(p3, n_filters * 8, kernel_size = 3)\n    p4 = tkl.MaxPooling2D((2,2))(c4)\n    p4 = tkl.Dropout(dropout_prob)(p4)\n    \n    c5 = conv_block(p4, n_filters * 16 , kernel_size = 3)\n    \n    ''' UpSampling '''\n    \n    u6 = conv_transpose_block(n_filters * 8, kernel_size = 3)(c5)  #----> 256\n    u6 = tkl.concatenate([u6, c4])     #----> 512\n    u6 = tkl.Dropout(dropout_prob)(u6)\n    c6 = conv_block(u6, n_filters * 8, kernel_size = 3)   #----> 256\n    \n    u7 = conv_transpose_block(n_filters * 4)(c6)   #----> 128\n    u7 = tkl.concatenate([u7, c3])   #----> 256\n    u7 = tkl.Dropout(dropout_prob)(u7)\n    c7 = conv_block(u7, n_filters * 4, kernel_size = 3)   #----> 128\n    \n    u8 = conv_transpose_block(n_filters * 2, kernel_size = 3)(c7)   #----> 64\n    u8 = tkl.concatenate([u8, c2])   #----> 128\n    u8 = tkl.Dropout(dropout_prob)(u8)\n    c8 = conv_block(u8, n_filters * 2, kernel_size = 3)   #----> 64\n    \n    u9 = conv_transpose_block(n_filters, kernel_size = 3)(c8)   #----> 32\n    u9 = tkl.concatenate([u9, c1])   #----> 64\n    u9 = tkl.Dropout(dropout_prob)(u9)\n    c9 = conv_block(u9, n_filters , kernel_size = 3)   #----> 32\n    \n    output = tkl.Conv2D(3, (1,1), activation = 'sigmoid')(c9)\n    \n    model = tf.keras.Model(inputs = [input_img], outputs = [output])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:04:40.625037Z","iopub.execute_input":"2021-12-19T07:04:40.625287Z","iopub.status.idle":"2021-12-19T07:04:40.645231Z","shell.execute_reply.started":"2021-12-19T07:04:40.625259Z","shell.execute_reply":"2021-12-19T07:04:40.644465Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# model = create_model()\nmodel = u_net()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:04:42.212624Z","iopub.execute_input":"2021-12-19T07:04:42.213159Z","iopub.status.idle":"2021-12-19T07:04:42.606057Z","shell.execute_reply.started":"2021-12-19T07:04:42.213124Z","shell.execute_reply":"2021-12-19T07:04:42.605359Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer = 'adam', loss = 'mse', metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:04:50.741226Z","iopub.execute_input":"2021-12-19T07:04:50.741481Z","iopub.status.idle":"2021-12-19T07:04:50.752456Z","shell.execute_reply.started":"2021-12-19T07:04:50.741454Z","shell.execute_reply":"2021-12-19T07:04:50.751639Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# gaussian_early_stop = EarlyStopping(monitor='loss', patience=3)\n# x, y = train_generator\nhistory = model.fit(train_generator,\n                              validation_data = val_generator,\n                              use_multiprocessing = True,\n                              workers = 6, epochs=50)\n# , callbacks=[gaussian_early_stop]\n\n# history = model.fit(train_generator, epochs=50)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:04:51.966517Z","iopub.execute_input":"2021-12-19T07:04:51.967053Z","iopub.status.idle":"2021-12-19T07:05:33.431138Z","shell.execute_reply.started":"2021-12-19T07:04:51.967015Z","shell.execute_reply":"2021-12-19T07:05:33.429631Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"model.save('UnetDenoising.h5')","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:04:00.260641Z","iopub.status.idle":"2021-12-19T07:04:00.261105Z","shell.execute_reply.started":"2021-12-19T07:04:00.260859Z","shell.execute_reply":"2021-12-19T07:04:00.260884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/stanford-cars-dataset/cars_test/cars_test/'\ntest_data = list(glob.glob(path + '*.jpg'))\n# print(len(test_data))\n\nevaluation_set = test_data[:64]\ntest_img_path = test_data[104]\ntest_img = cv2.imread(test_img_path)\ntest_img = cv2.resize(test_img, (WIDTH, HEIGHT)) / 255\ntest_img = random_noise(test_img, mode = 'gaussian')\nplt.imshow(test_img)\nprint(test_img.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:04:01.682402Z","iopub.execute_input":"2021-12-19T07:04:01.682664Z","iopub.status.idle":"2021-12-19T07:04:01.706451Z","shell.execute_reply.started":"2021-12-19T07:04:01.682634Z","shell.execute_reply":"2021-12-19T07:04:01.705587Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"model.predict(test_img)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_set = []\nfor i in range(64):\n    img = cv2.imread(evaluation_set[i])\n    img = cv2.resize(img, (WIDTH, HEIGHT)) / 255\n    img = random_noise(img, mode = 'gaussian')\n    eval_set.append(img)\neval_set = np.array(eval_set)\n\n# test_generator = DataGenerator(evaluation_set)\n# x, y = test_generator[2]\npredicted = model.predict(eval_set)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.imshow(predicted[10])\npredicted_ = cv2.resize(predicted[11], (500, 500))\n# fig, ax = plt.subplots(1,3)\nprepared_img = cv2.imread(evaluation_set[11]) / 255\nprepared_img = cv2.resize(prepared_img, (500, 500))\nnoisy_orig = random_noise(prepared_img, mode = 'gaussian')\n# ax[0].imshow(prepared_img)\n# ax[1].imshow(predicted_)\n# ax[2].imshow(noisy_orig)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}